{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Project - Complete Tutorial\n",
        "\n",
        "This notebook provides a comprehensive tutorial on using T5 models, based on `solution-01-t5.ipynb`.\n",
        "\n",
        "**What you'll learn:**\n",
        "- Tokenization: Converting text to numbers and back\n",
        "- Model architecture: Encoder-decoder structure, shared embeddings\n",
        "- Encoder outputs: Understanding how input is processed\n",
        "- Manual generation: Step-by-step autoregressive token generation\n",
        "- Optimized generation: Using model.generate() with caching\n",
        "- Embeddings visualization: PCA and cosine similarity\n",
        "\n",
        "**Prerequisites:** None! This notebook installs everything you need.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 3) (3212902300.py, line 3)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mFirst, we'll clone the repository and install all dependencies.\u001b[39m\n             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 3)\n"
          ]
        }
      ],
      "source": [
        "## Step 1: Setup and Installation\n",
        "\n",
        "First, we'll clone the repository and install all dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/SabraHashemi/llm-project.git\n",
        "%cd llm-project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install required packages for transformers, visualization, and machine learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q transformers torch matplotlib scikit-learn numpy python-dateutil\n",
        "print(\"‚úÖ Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llm_tokenizers'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Add repo root to path if needed\u001b[39;00m\n\u001b[32m      9\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# adjust if your module path differs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_tokenizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseTokenizerWrapper\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Seq2SeqModelLoader\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Imports ready\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'llm_tokenizers'"
          ]
        }
      ],
      "source": [
        "## Step 2: Import Modules\n",
        "\n",
        "Import the necessary libraries and our custom tokenizer/model loader modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Add project root to Python path\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "# Import our custom modules\n",
        "from llm_tokenizers import BaseTokenizerWrapper\n",
        "from llm_models import Seq2SeqModelLoader\n",
        "\n",
        "print(\"‚úÖ All modules imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 3: Initialize Tokenizer and Model\n",
        "\n",
        "Load the T5-small tokenizer and model. This will download the models on first run (~240MB).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = BaseTokenizerWrapper(\"t5-small\")\n",
        "print(f\"‚úÖ Tokenizer loaded! Vocabulary size: {tokenizer.vocab_size:,} tokens\")\n",
        "\n",
        "# Initialize model\n",
        "print(\"\\nLoading model (this may take a moment on first run)...\")\n",
        "model = Seq2SeqModelLoader(\"t5-small\")\n",
        "print(f\"‚úÖ Model loaded!\")\n",
        "print(f\"\\nüìä Model Configuration:\")\n",
        "print(f\"   - Hidden size (d_model): {model.hidden_size}\")\n",
        "print(f\"   - Number of layers: {model.num_layers}\")\n",
        "print(f\"   - Number of attention heads: {model.num_heads}\")\n",
        "print(f\"   - Feed-forward dimension (d_ff): {model.hidden_size * 4}\")  # T5 uses 4x expansion\n",
        "print(f\"   - Vocabulary size: {model.vocab_size:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Understanding Model Architecture\n",
        "\n",
        "**T5 Architecture Overview:**\n",
        "- **Encoder**: Processes input text (bidirectional self-attention + feed-forward)\n",
        "- **Decoder**: Generates output text (self-attention + cross-attention + feed-forward)\n",
        "- **Shared Embeddings**: Encoder and decoder share the same token embedding layer\n",
        "- **Language Model Head (lm_head)**: Maps decoder output to vocabulary probabilities\n",
        "\n",
        "**What is tokenization?**\n",
        "- **Encoding**: Converts human-readable text ‚Üí token IDs (numbers)\n",
        "- **Decoding**: Converts token IDs (numbers) ‚Üí human-readable text\n",
        "\n",
        "This is the fundamental conversion between text and the numerical representation models use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect model architecture\n",
        "print(\"üîç Model Structure:\")\n",
        "print(f\"   - Encoder: {len(model.model.encoder.block)} layers\")\n",
        "print(f\"   - Decoder: {len(model.model.decoder.block)} layers\")\n",
        "print(f\"   - Shared embeddings: {model.model.shared}\")\n",
        "print(f\"   - Language model head: {model.model.lm_head}\")\n",
        "\n",
        "# Verify shared embeddings\n",
        "is_shared = (id(model.model.shared) == id(model.model.encoder.embed_tokens) and \n",
        "             id(model.model.shared) == id(model.model.decoder.embed_tokens))\n",
        "print(f\"\\n‚úÖ Shared embeddings: {is_shared}\")\n",
        "print(\"   ‚Üí Encoder and decoder use the same embedding weights!\")\n",
        "\n",
        "# Inspect encoder block structure\n",
        "print(f\"\\nüì¶ Encoder Block Structure (first layer):\")\n",
        "encoder_block = model.model.encoder.block[0]\n",
        "print(f\"   1. Self-Attention: {type(encoder_block.layer[0]).__name__}\")\n",
        "print(f\"   2. Feed-Forward: {type(encoder_block.layer[1]).__name__}\")\n",
        "\n",
        "# Inspect decoder block structure\n",
        "print(f\"\\nüì¶ Decoder Block Structure (first layer):\")\n",
        "decoder_block = model.model.decoder.block[0]\n",
        "print(f\"   1. Self-Attention: {type(decoder_block.layer[0]).__name__}\")\n",
        "print(f\"   2. Cross-Attention: {type(decoder_block.layer[1]).__name__}\")\n",
        "print(f\"   3. Feed-Forward: {type(decoder_block.layer[2]).__name__}\")\n",
        "print(\"\\nüí° Key difference: Decoder has cross-attention to look at encoder outputs!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Encode text to token IDs\n",
        "sentence = \"hello, this is a sentence!\"\n",
        "tokens = tokenizer.encode(sentence)\n",
        "\n",
        "print(f\"Original text: '{sentence}'\")\n",
        "print(f\"Token IDs: {tokens['input_ids']}\")\n",
        "print(f\"Attention mask: {tokens['attention_mask']}\")\n",
        "print(f\"\\nDecoded back: '{tokenizer.decode(tokens['input_ids'])}'\")\n",
        "print(\"\\nüí° Note: The tokenizer automatically added </s> (end-of-sequence token)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# See actual tokens (not just IDs)\n",
        "sentence = \"hello, this is a sentence!\"\n",
        "tokens_list = tokenizer.tokenize(sentence)\n",
        "print(f\"Original: '{sentence}'\")\n",
        "print(f\"Tokens: {tokens_list}\")\n",
        "print(\"\\nüí° The '‚ñÅ' prefix indicates a word starting after a space!\")\n",
        "\n",
        "# Compare tokenization with/without spaces\n",
        "print(\"\\nüìù Space matters in tokenization:\")\n",
        "print(f\"  'hello,world' ‚Üí {tokenizer.tokenize('hello,world')}\")\n",
        "print(f\"  'hello, world' ‚Üí {tokenizer.tokenize('hello, world')}\")\n",
        "print(\"\\nüí° Notice how 'world' is tokenized differently!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Special Tokens\n",
        "\n",
        "Each model uses special tokens with specific meanings:\n",
        "- **EOS** (`</s>`): End of sequence\n",
        "- **PAD** (`<pad>`): Padding token (also used as decoder start in T5)\n",
        "- **BOS**: Beginning of sequence (T5 doesn't use this)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Encoding with Padding\n",
        "\n",
        "**Why padding?**\n",
        "When processing multiple sentences of different lengths, we need to pad shorter sentences to match the longest one. This allows us to stack them into a tensor.\n",
        "\n",
        "**Attention masks:** Tell the model which tokens are real (1) and which are padding (0).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch encoding example\n",
        "sentences = [\n",
        "    \"this is the first sentence\",\n",
        "    \"instead, this is the second sequence!\"\n",
        "]\n",
        "\n",
        "# Without padding\n",
        "tokens_no_pad = tokenizer.encode(sentences)\n",
        "print(\"Without padding:\")\n",
        "for i, sent in enumerate(sentences):\n",
        "    print(f\"  {i+1}. Length: {len(tokens_no_pad['input_ids'][i])} tokens\")\n",
        "\n",
        "# With padding\n",
        "tokens_padded = tokenizer.encode(sentences, padding=True)\n",
        "print(\"\\nWith padding:\")\n",
        "for i, (ids, mask) in enumerate(zip(tokens_padded['input_ids'], tokens_padded['attention_mask'])):\n",
        "    print(f\"  {i+1}. IDs: {ids}\")\n",
        "    print(f\"     Mask: {mask}\")\n",
        "    print(f\"     ‚Üí Mask 0 = padding (model ignores these)\")\n",
        "\n",
        "print(\"\\nüí° All sentences now have the same length!\")\n",
        "print(\"üí° Attention mask prevents the model from attending to padding tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get special tokens\n",
        "special = tokenizer.get_special_tokens()\n",
        "print(\"Special tokens for T5:\")\n",
        "print(f\"  EOS token: '{special['eos_token']}' (ID: {special['eos_token_id']})\")\n",
        "print(f\"  PAD token: '{special['pad_token']}' (ID: {special['pad_token_id']})\")\n",
        "print(f\"  BOS token: {special['bos_token']} (T5 doesn't use BOS)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Encoder Outputs\n",
        "\n",
        "**What does the encoder do?**\n",
        "The encoder processes the input text and creates a rich representation that the decoder uses to generate output.\n",
        "\n",
        "**Key outputs:**\n",
        "- `encoder_last_hidden_state`: Final hidden states from all encoder layers\n",
        "- This representation stays constant during generation (encoder runs once)\n",
        "- Decoder uses this via cross-attention to generate relevant output\n",
        "\n",
        "**How T5 works:**\n",
        "1. **Encoder**: Processes the input text (e.g., \"translate english to german: hello\")\n",
        "2. **Decoder**: Generates output tokens one by one\n",
        "3. **First step**: Decoder starts with `<pad>` token (T5's special start token)\n",
        "4. **Output**: Model produces logits (probabilities) for each possible next token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare input and run encoder\n",
        "input_sentence = \"translate english to german: hello, how are you?\"\n",
        "tokens = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
        "decoder_input_ids = torch.tensor([[tokenizer.tokenizer.pad_token_id]])\n",
        "\n",
        "print(f\"Input: '{input_sentence}'\")\n",
        "print(f\"Token IDs: {tokens['input_ids'].tolist()[0]}\")\n",
        "print(f\"Sequence length: {tokens['input_ids'].shape[1]} tokens\\n\")\n",
        "\n",
        "# Forward pass to get encoder outputs\n",
        "with torch.no_grad():\n",
        "    output = model(**tokens, decoder_input_ids=decoder_input_ids)\n",
        "\n",
        "# Inspect encoder outputs\n",
        "encoder_hidden = output.encoder_last_hidden_state\n",
        "print(f\"üì§ Encoder Output:\")\n",
        "print(f\"   Shape: {encoder_hidden.shape}\")\n",
        "print(f\"   ‚Üí [batch=1, sequence_length={encoder_hidden.shape[1]}, hidden_size={encoder_hidden.shape[2]}]\")\n",
        "print(f\"\\nüí° This representation captures the meaning of the entire input sentence!\")\n",
        "print(f\"üí° The decoder will use this via cross-attention to generate the translation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare input\n",
        "input_sentence = \"translate english to german: hello, how are you?\"\n",
        "tokens = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
        "decoder_input_ids = torch.tensor([[tokenizer.tokenizer.pad_token_id]])\n",
        "\n",
        "print(f\"Input: '{input_sentence}'\")\n",
        "print(f\"Encoder input shape: {tokens['input_ids'].shape}\")\n",
        "print(f\"Decoder input (starting token): {decoder_input_ids.tolist()}\")\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    output = model(**tokens, decoder_input_ids=decoder_input_ids)\n",
        "\n",
        "print(f\"\\n‚úÖ Forward pass completed!\")\n",
        "print(f\"Output logits shape: {output.logits.shape}\")\n",
        "print(f\"   ‚Üí Shape means: [batch=1, sequence=1, vocab={model.vocab_size:,}]\")\n",
        "print(f\"   ‚Üí For each position, we have {model.vocab_size:,} logits (one per token)\")\n",
        "print(f\"\\nOutput contains:\")\n",
        "print(f\"  - logits: Probabilities for next token\")\n",
        "print(f\"  - past_key_values: Cached attention states\")\n",
        "print(f\"  - encoder_last_hidden_state: Final encoder representation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual step-by-step generation\n",
        "input_sentence = \"translate english to german: hello, how are you?\"\n",
        "tokens = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
        "decoder_input_ids = torch.tensor([[tokenizer.tokenizer.pad_token_id]])\n",
        "\n",
        "print(f\"Input: '{input_sentence}'\\n\")\n",
        "print(\"üîÑ Manual Generation Process:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "max_length = 10\n",
        "i = 0\n",
        "\n",
        "while i < max_length and decoder_input_ids[0, -1].item() != tokenizer.tokenizer.eos_token_id:\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        output = model(**tokens, decoder_input_ids=decoder_input_ids)\n",
        "    \n",
        "    # Get the most likely next token (greedy decoding)\n",
        "    next_token_logits = output.logits[0, -1, :]  # Last position, all vocab\n",
        "    next_token_id = next_token_logits.argmax().item()\n",
        "    \n",
        "    # Decode current sequence\n",
        "    current_text = tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)\n",
        "    next_token_text = tokenizer.decode([next_token_id], skip_special_tokens=True)\n",
        "    \n",
        "    print(f\"Step {i+1}: '{current_text}' ‚Üí next token: '{next_token_text}' (ID: {next_token_id})\")\n",
        "    \n",
        "    # Add predicted token to decoder input\n",
        "    decoder_input_ids = torch.cat([decoder_input_ids, torch.tensor([[next_token_id]])], dim=1)\n",
        "    \n",
        "    # Check if we hit end token\n",
        "    if next_token_id == tokenizer.tokenizer.eos_token_id:\n",
        "        print(f\"\\n‚úÖ Generation complete! Final output: '{tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)}'\")\n",
        "        break\n",
        "    \n",
        "    i += 1\n",
        "\n",
        "print(\"\\nüí° Key insights:\")\n",
        "print(\"   - Each step predicts ONE token at a time\")\n",
        "print(\"   - The model uses ALL previous tokens + encoder output\")\n",
        "print(\"   - Generation stops when <eos> token is predicted\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Text Generation\n",
        "\n",
        "**Autoregressive Generation:**\n",
        "Models generate text token by token. Each new token depends on all previous tokens.\n",
        "\n",
        "**Greedy Decoding:** Always picks the most likely next token (highest probability).\n",
        "\n",
        "The `model.generate()` method does this automatically with optimizations!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimized generation using model.generate()\n",
        "input_sentence = \"translate english to german: hello, how are you?\"\n",
        "tokens = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
        "\n",
        "print(f\"Input:  '{input_sentence}'\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(**tokens, max_length=20)\n",
        "\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Output: '{generated_text}'\")\n",
        "print(f\"\\n‚úÖ Generated in one call!\")\n",
        "print(f\"üí° model.generate() is optimized with caching and is much faster than manual loops.\")\n",
        "print(f\"üí° The encoder runs once, and its output is reused for all decoder steps.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text using model.generate()\n",
        "# Note: We need to prepare tokens again for generation\n",
        "input_sentence = \"translate english to german: hello, how are you?\"\n",
        "tokens = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(**tokens, max_length=20)\n",
        "\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Input:  '{input_sentence}'\")\n",
        "print(f\"Output: '{generated_text}'\")\n",
        "print(f\"\\nüí° The model translated English to German!\")\n",
        "print(f\"üí° model.generate() is optimized and faster than manual generation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Token Embeddings Visualization\n",
        "\n",
        "**What are embeddings?**\n",
        "- Each word/token is represented as a high-dimensional vector (512 dimensions for T5-small)\n",
        "- These vectors capture semantic meaning\n",
        "- Similar words have similar embeddings\n",
        "\n",
        "**Visualization techniques:**\n",
        "1. **PCA (Principal Component Analysis)**: Reduces 512D ‚Üí 2D for visualization\n",
        "2. **Cosine Similarity**: Measures how \"similar\" two word embeddings are (0-1 scale)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Words to visualize\n",
        "words = [\n",
        "    \"chair\",\n",
        "    \"table\",\n",
        "    \"plate\",\n",
        "    \"knife\",\n",
        "    \"spoon\",\n",
        "    \"horse\",\n",
        "    \"goat\",\n",
        "    \"sheep\",\n",
        "    \"cat\",\n",
        "    \"dog\",\n",
        "]\n",
        "\n",
        "print(f\"üìù Analyzing embeddings for {len(words)} words:\")\n",
        "for i, word in enumerate(words, 1):\n",
        "    print(f\"   {i}. {word}\")\n",
        "\n",
        "# Get token IDs for first token of each word\n",
        "word_tokens = tokenizer.encode(words, return_tensors=\"pt\", padding=True)[\"input_ids\"][:, 0]\n",
        "print(f\"\\nToken IDs: {word_tokens.tolist()}\")\n",
        "\n",
        "# Extract embeddings from the model's shared embedding layer\n",
        "with torch.no_grad():\n",
        "    token_embeddings = model.model.shared(word_tokens).cpu().detach().numpy()\n",
        "\n",
        "print(f\"‚úÖ Embeddings extracted!\")\n",
        "print(f\"   Shape: {token_embeddings.shape}\")\n",
        "print(f\"   ‚Üí Each word is a {token_embeddings.shape[1]}-dimensional vector\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PCA Visualization (2D Projection)\n",
        "\n",
        "PCA reduces high-dimensional embeddings to 2D so we can visualize them. Words that are semantically similar should appear close together!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply PCA to reduce dimensions from 512D to 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(token_embeddings)\n",
        "\n",
        "print(f\"Explained variance per component: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n",
        "print(\"‚Üí This shows how much information is preserved in 2D\")\n",
        "\n",
        "# Create PCA plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], s=200, alpha=0.7, edgecolors='black', linewidth=1.5)\n",
        "\n",
        "# Add labels with better positioning\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, (X_pca[i, 0], X_pca[i, 1]), \n",
        "                xytext=(5, 5), textcoords='offset points', \n",
        "                fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.xlabel('First Principal Component', fontsize=13)\n",
        "plt.ylabel('Second Principal Component', fontsize=13)\n",
        "plt.title('Token Embeddings - PCA Visualization (2D Projection)\\nWords close together are semantically similar', \n",
        "          fontsize=15, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3, linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Observations:\")\n",
        "print(\"   - Furniture words (chair, table, plate, knife, spoon) should cluster together\")\n",
        "print(\"   - Animal words (horse, goat, sheep, cat, dog) should cluster together\")\n",
        "print(\"   - This shows the model learned semantic relationships!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Cross-Attention Visualization (Optional)\n",
        "\n",
        "**What is cross-attention?**\n",
        "Cross-attention shows what parts of the input the decoder focuses on when generating each output token.\n",
        "\n",
        "**Key insights:**\n",
        "- Early layers focus on task identification (\"translate\", \"german\")\n",
        "- Later layers focus on content words (\"hello\")\n",
        "- This reveals how the model connects input to output!\n",
        "\n",
        "**Note:** This requires reloading the model with `output_attentions=True`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload model with attention outputs enabled\n",
        "print(\"Reloading model with attention outputs...\")\n",
        "model_with_attn = Seq2SeqModelLoader(\"t5-small\", output_attentions=True)\n",
        "\n",
        "# Prepare input\n",
        "input_sentence = \"translate english to german: hello how are you?\"\n",
        "tokens = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
        "decoder_input_ids = torch.tensor([[tokenizer.tokenizer.pad_token_id]])\n",
        "\n",
        "# Forward pass with attention\n",
        "with torch.no_grad():\n",
        "    output = model_with_attn(**tokens, decoder_input_ids=decoder_input_ids)\n",
        "\n",
        "print(f\"\\n‚úÖ Model outputs now include attention weights!\")\n",
        "print(f\"Available keys: {list(output.keys())}\")\n",
        "print(f\"\\nCross-attention layers: {len(output.cross_attentions)}\")\n",
        "print(f\"Shape of first cross-attention: {output.cross_attentions[0].shape}\")\n",
        "print(f\"   ‚Üí [batch=1, heads={output.cross_attentions[0].shape[1]}, decoder_pos=1, encoder_pos={output.cross_attentions[0].shape[3]}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cross-attention for first layer (averaged across heads)\n",
        "first_layer_attn = output.cross_attentions[0][0, :, 0].detach().cpu().numpy()  # [heads, encoder_pos]\n",
        "avg_attn = first_layer_attn.mean(axis=0)  # Average across heads\n",
        "\n",
        "# Get input tokens for labels\n",
        "input_tokens = tokenizer.tokenize(input_sentence) + [\"</s>\"]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(len(input_tokens)), avg_attn)\n",
        "plt.yticks(range(len(input_tokens)), input_tokens)\n",
        "plt.xlabel('Attention Weight (Average across heads)')\n",
        "plt.title('Cross-Attention: First Layer\\n(What the decoder focuses on when generating first token)')\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Higher bars = more attention\")\n",
        "print(\"üí° Early layers typically focus on task words ('translate', 'german')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vocabulary Exploration\n",
        "\n",
        "The tokenizer has a vocabulary mapping tokens to IDs. Let's explore it!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get vocabulary\n",
        "vocabulary = tokenizer.get_vocab()\n",
        "reverse_vocab = {v: k for k, v in vocabulary.items()}\n",
        "\n",
        "print(f\"üìö Vocabulary size: {len(vocabulary):,} tokens\")\n",
        "print(f\"   ‚Üí 32,000 base tokens + 100 special tokens (<extra_id_0> to <extra_id_99>)\")\n",
        "\n",
        "# Show some random tokens\n",
        "import random\n",
        "random_tokens = random.sample(list(vocabulary.keys()), 10)\n",
        "print(f\"\\nüîÄ Random tokens from vocabulary:\")\n",
        "for token in random_tokens:\n",
        "    print(f\"   '{token}' ‚Üí ID: {vocabulary[token]}\")\n",
        "\n",
        "# Check special tokens\n",
        "print(f\"\\nüéØ Special token IDs:\")\n",
        "print(f\"   EOS (</s>): {vocabulary.get('</s>', 'N/A')}\")\n",
        "print(f\"   PAD (<pad>): {vocabulary.get('<pad>', 'N/A')}\")\n",
        "print(f\"   Extra ID 0: {vocabulary.get('<extra_id_0>', 'N/A')}\")\n",
        "print(f\"   Extra ID 1: {vocabulary.get('<extra_id_1>', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Attention Across All Layers\n",
        "\n",
        "Let's visualize cross-attention across all decoder layers to see how attention patterns change from early to late layers.\n",
        "\n",
        "**Note:** This requires reloading the model with `output_attentions=True`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload model with attention outputs enabled\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model_with_attn = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\", output_attentions=True)\n",
        "model_with_attn.eval()\n",
        "\n",
        "# Prepare input\n",
        "input_sentence = \"translate english to german: hello how are you?\"\n",
        "tokens = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
        "decoder_input_ids = torch.tensor([[tokenizer.tokenizer.pad_token_id]])\n",
        "\n",
        "# Forward pass to get attention\n",
        "with torch.no_grad():\n",
        "    output = model_with_attn(**tokens, decoder_input_ids=decoder_input_ids)\n",
        "\n",
        "# Visualize cross-attention across all decoder layers\n",
        "num_layers = len(output.cross_attentions)\n",
        "fig, axes = plt.subplots(1, min(num_layers, 10), figsize=(14, 3))\n",
        "\n",
        "if num_layers == 1:\n",
        "    axes = [axes]  # Make it iterable if only one layer\n",
        "\n",
        "axes[0].set_ylabel(\"Attention head\", fontsize=11)\n",
        "\n",
        "input_tokens = tokenizer.tokenize(input_sentence) + [\"</s>\"]\n",
        "\n",
        "for i in range(min(num_layers, 10)):\n",
        "    # Get attention for layer i: [batch=1, heads, decoder_pos=1, encoder_pos]\n",
        "    layer_attn = output.cross_attentions[i][0, :, 0].detach().cpu().numpy()\n",
        "    \n",
        "    # Plot heatmap\n",
        "    im = axes[i].imshow(layer_attn, aspect='auto', cmap='viridis')\n",
        "    axes[i].set_xticks(range(len(input_tokens)))\n",
        "    axes[i].set_xticklabels(input_tokens, rotation=90, fontsize=9)\n",
        "    axes[i].set_yticks([])\n",
        "    axes[i].set_title(f\"Layer {i+1}\", fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Cross-Attention Across Decoder Layers\\n(Each layer shows attention from all heads)', \n",
        "             fontsize=13, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Observations:\")\n",
        "print(\"   - Early layers (1-3): Focus on task words ('translate', 'german')\")\n",
        "print(\"   - Middle layers (4-7): Transition to content words\")\n",
        "print(\"   - Late layers (8-10): Focus on actual content ('hello', 'how', 'are')\")\n",
        "print(\"   - This shows how the model processes information hierarchically!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attention Throughout Generation\n",
        "\n",
        "Now let's see how attention shifts as the model generates each token. This shows how the decoder focuses on different parts of the input as it generates different parts of the output!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate the full sequence step-by-step and collect attention weights\n",
        "input_sentence = \"translate english to german: hello how are you?\"\n",
        "tokens = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
        "decoder_input_ids = torch.tensor([[tokenizer.tokenizer.pad_token_id]])\n",
        "\n",
        "attns = []\n",
        "\n",
        "max_length = 20\n",
        "i = 0\n",
        "\n",
        "print(\"üîÑ Generating sequence and collecting attention weights...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "while i < max_length and decoder_input_ids[0, -1].item() != tokenizer.tokenizer.eos_token_id:\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        step_output = model_with_attn(**tokens, decoder_input_ids=decoder_input_ids)\n",
        "    \n",
        "    # Get predicted token (greedy decoding)\n",
        "    next_token_logits = step_output.logits[0, -1, :]\n",
        "    next_token_id = next_token_logits.argmax().item()\n",
        "    \n",
        "    # Store attention weights for this step\n",
        "    attns.append(step_output.cross_attentions)\n",
        "    \n",
        "    # Decode current sequence\n",
        "    current_text = tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)\n",
        "    next_token_text = tokenizer.decode([next_token_id], skip_special_tokens=True)\n",
        "    \n",
        "    print(f\"Step {i+1}: '{current_text}' ‚Üí '{next_token_text}'\")\n",
        "    \n",
        "    # Add predicted token\n",
        "    decoder_input_ids = torch.cat([decoder_input_ids, torch.tensor([[next_token_id]])], dim=1)\n",
        "    \n",
        "    if next_token_id == tokenizer.tokenizer.eos_token_id:\n",
        "        break\n",
        "    \n",
        "    i += 1\n",
        "\n",
        "print(f\"\\n‚úÖ Generated {i} tokens\")\n",
        "print(f\"‚úÖ Collected attention weights for each generation step\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create heatmap showing attention throughout generation\n",
        "# Average attention across all layers and heads, for the last decoder position at each step\n",
        "attention_matrix = torch.stack([\n",
        "    torch.stack(a).mean(axis=(0, 1, 2))[-1]  # Average across layers and heads, last decoder pos\n",
        "    for a in attns\n",
        "]).detach().cpu().numpy()\n",
        "\n",
        "input_tokens = tokenizer.tokenize(input_sentence) + [\"</s>\"]\n",
        "\n",
        "# Get the final generated sequence tokens\n",
        "output_tokens = tokenizer.tokenize(tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "im = ax.imshow(attention_matrix, cmap='YlOrRd', aspect='auto', interpolation='nearest')\n",
        "\n",
        "# Set labels\n",
        "ax.set_xticks(range(len(input_tokens)))\n",
        "ax.set_xticklabels(input_tokens, rotation=45, ha='right', fontsize=10)\n",
        "ax.set_yticks(range(len(output_tokens)))\n",
        "ax.set_yticklabels(output_tokens, fontsize=10)\n",
        "\n",
        "ax.set_xlabel('Input Token', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Generated Token', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Attention Throughout Generation\\n(How decoder focuses on input as it generates output)', \n",
        "             fontsize=14, fontweight='bold', pad=15)\n",
        "\n",
        "# Add colorbar\n",
        "cbar = plt.colorbar(im, ax=ax, label='Attention Weight', shrink=0.8)\n",
        "cbar.ax.tick_params(labelsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Key insights:\")\n",
        "print(\"   - When generating 'Hallo', attention focuses on 'hello'\")\n",
        "print(\"   - As generation progresses, attention shifts to other input words\")\n",
        "print(\"   - This shows the decoder dynamically attends to different parts of the input!\")\n",
        "print(\"   - The model learns to align input and output words semantically!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cosine Similarity Matrix\n",
        "\n",
        "Cosine similarity measures how similar two word embeddings are:\n",
        "- **1.0** = Identical (same word)\n",
        "- **0.8-1.0** = Very similar (e.g., \"cat\" and \"dog\")\n",
        "- **0.5-0.8** = Somewhat related\n",
        "- **0.0-0.5** = Unrelated\n",
        "- **Negative** = Opposites or very different\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute cosine similarity matrix\n",
        "similarity_matrix = cosine_similarity(token_embeddings)\n",
        "\n",
        "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
        "print(f\"‚Üí Shows pairwise similarity between all {len(words)} words\\n\")\n",
        "\n",
        "# Create heatmap with better styling\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "cax = ax.imshow(similarity_matrix, cmap=\"RdYlBu_r\", vmin=-1, vmax=1, aspect='auto')\n",
        "cbar = fig.colorbar(cax, ax=ax, label='Cosine Similarity', shrink=0.8)\n",
        "cbar.ax.tick_params(labelsize=11)\n",
        "\n",
        "ax.set_xticks(range(len(words)))\n",
        "ax.set_yticks(range(len(words)))\n",
        "ax.set_xticklabels(words, rotation=45, ha='right', fontsize=11)\n",
        "ax.set_yticklabels(words, fontsize=11)\n",
        "ax.set_title('Cosine Similarity Matrix\\n(Red = similar, Blue = different)', \n",
        "             fontsize=15, fontweight='bold', pad=20)\n",
        "\n",
        "# Add similarity values to the plot (only show if similarity > 0.3 for readability)\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        sim_val = similarity_matrix[i, j]\n",
        "        # Use white text for dark backgrounds, black for light\n",
        "        text_color = 'white' if abs(sim_val) > 0.5 else 'black'\n",
        "        if abs(sim_val) > 0.3 or i == j:  # Show diagonal and significant similarities\n",
        "            ax.text(j, i, f'{sim_val:.2f}',\n",
        "                   ha=\"center\", va=\"center\", \n",
        "                   color=text_color, fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print most similar pairs\n",
        "print(\"\\nüí° Most similar word pairs:\")\n",
        "similarities = []\n",
        "for i in range(len(words)):\n",
        "    for j in range(i + 1, len(words)):\n",
        "        similarities.append((words[i], words[j], similarity_matrix[i, j]))\n",
        "\n",
        "similarities.sort(key=lambda x: x[2], reverse=True)\n",
        "for i, (word1, word2, sim) in enumerate(similarities[:5], 1):\n",
        "    print(f\"   {i}. '{word1}' ‚Üî '{word2}': {sim:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**What you learned:**\n",
        "1. ‚úÖ **Tokenization**: Converting text to numbers (token IDs) and back\n",
        "2. ‚úÖ **Model Architecture**: Encoder-decoder structure, shared embeddings, attention layers\n",
        "3. ‚úÖ **Encoder Outputs**: How the encoder processes input and creates representations\n",
        "4. ‚úÖ **Manual Generation**: Step-by-step autoregressive token generation process\n",
        "5. ‚úÖ **Optimized Generation**: Using `model.generate()` with caching for efficiency\n",
        "6. ‚úÖ **Embeddings**: How words are represented as high-dimensional vectors\n",
        "7. ‚úÖ **Visualization**: PCA and cosine similarity for understanding semantic relationships\n",
        "\n",
        "**Key Concepts:**\n",
        "- **Encoder**: Processes input once, creates rich representation (bidirectional)\n",
        "- **Decoder**: Generates output token-by-token (autoregressive) using cross-attention\n",
        "- **Shared Embeddings**: Encoder and decoder share the same token embedding weights\n",
        "- **Autoregressive**: Each token depends on all previous tokens\n",
        "- **Caching**: Encoder outputs are cached during generation for efficiency\n",
        "\n",
        "**Next steps:**\n",
        "- Try different input prompts and tasks (summarization, question answering)\n",
        "- Experiment with different T5 model sizes (t5-base, t5-large)\n",
        "- Explore attention mechanisms in `solution-02-attention.ipynb`\n",
        "- Modify the code to experiment with different models (BERT, GPT-2, etc.)\n",
        "- Study cross-attention weights to see what the model focuses on\n",
        "\n",
        "**Resources:**\n",
        "- Full tutorial: `labs/solution-01-t5.ipynb`\n",
        "- Test suite: `test_t5_notebook.py`\n",
        "- More examples: `examples/` folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venvllm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
