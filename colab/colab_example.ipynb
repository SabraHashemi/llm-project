{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Project - Complete Tutorial\n",
        "\n",
        "This notebook provides a comprehensive tutorial on using T5 models, based on `solution-01-t5.ipynb`.\n",
        "\n",
        "**What you'll learn:**\n",
        "- Tokenization: Converting text to numbers and back\n",
        "- Model loading: Understanding T5 architecture\n",
        "- Text generation: How models create output\n",
        "- Embeddings visualization: PCA and cosine similarity\n",
        "- Forward passes: Encoder-decoder mechanics\n",
        "\n",
        "**Prerequisites:** None! This notebook installs everything you need.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 3) (3212902300.py, line 3)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mFirst, we'll clone the repository and install all dependencies.\u001b[39m\n             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 3)\n"
          ]
        }
      ],
      "source": [
        "## Step 1: Setup and Installation\n",
        "\n",
        "First, we'll clone the repository and install all dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/SabraHashemi/llm-project.git\n",
        "%cd llm-project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install required packages for transformers, visualization, and machine learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q transformers torch matplotlib scikit-learn numpy python-dateutil\n",
        "print(\"âœ… Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llm_tokenizers'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Add repo root to path if needed\u001b[39;00m\n\u001b[32m      9\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# adjust if your module path differs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_tokenizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseTokenizerWrapper\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Seq2SeqModelLoader\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Imports ready\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'llm_tokenizers'"
          ]
        }
      ],
      "source": [
        "## Step 2: Import Modules\n",
        "\n",
        "Import the necessary libraries and our custom tokenizer/model loader modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Add project root to Python path\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "# Import our custom modules\n",
        "from llm_tokenizers import BaseTokenizerWrapper\n",
        "from llm_models import Seq2SeqModelLoader\n",
        "\n",
        "print(\"âœ… All modules imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 3: Initialize Tokenizer and Model\n",
        "\n",
        "Load the T5-small tokenizer and model. This will download the models on first run (~240MB).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = BaseTokenizerWrapper(\"t5-small\")\n",
        "print(f\"âœ… Tokenizer loaded! Vocabulary size: {tokenizer.vocab_size:,} tokens\")\n",
        "\n",
        "# Initialize model\n",
        "print(\"\\nLoading model (this may take a moment on first run)...\")\n",
        "model = Seq2SeqModelLoader(\"t5-small\")\n",
        "print(f\"âœ… Model loaded!\")\n",
        "print(f\"   - Hidden size: {model.hidden_size}\")\n",
        "print(f\"   - Number of layers: {model.num_layers}\")\n",
        "print(f\"   - Number of attention heads: {model.num_heads}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Encoding and Decoding\n",
        "\n",
        "**What is tokenization?**\n",
        "- **Encoding**: Converts human-readable text â†’ token IDs (numbers)\n",
        "- **Decoding**: Converts token IDs (numbers) â†’ human-readable text\n",
        "\n",
        "This is the fundamental conversion between text and the numerical representation models use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Encode text to token IDs\n",
        "sentence = \"hello, this is a sentence!\"\n",
        "tokens = tokenizer.encode(sentence)\n",
        "\n",
        "print(f\"Original text: '{sentence}'\")\n",
        "print(f\"Token IDs: {tokens['input_ids']}\")\n",
        "print(f\"Attention mask: {tokens['attention_mask']}\")\n",
        "print(f\"\\nDecoded back: '{tokenizer.decode(tokens['input_ids'])}'\")\n",
        "print(\"\\nðŸ’¡ Note: The tokenizer automatically added </s> (end-of-sequence token)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Special Tokens\n",
        "\n",
        "Each model uses special tokens with specific meanings:\n",
        "- **EOS** (`</s>`): End of sequence\n",
        "- **PAD** (`<pad>`): Padding token (also used as decoder start in T5)\n",
        "- **BOS**: Beginning of sequence (T5 doesn't use this)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get special tokens\n",
        "special = tokenizer.get_special_tokens()\n",
        "print(\"Special tokens for T5:\")\n",
        "print(f\"  EOS token: '{special['eos_token']}' (ID: {special['eos_token_id']})\")\n",
        "print(f\"  PAD token: '{special['pad_token']}' (ID: {special['pad_token_id']})\")\n",
        "print(f\"  BOS token: {special['bos_token']} (T5 doesn't use BOS)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Forward Pass (Encoder-Decoder Architecture)\n",
        "\n",
        "**How T5 works:**\n",
        "1. **Encoder**: Processes the input text (e.g., \"translate english to german: hello\")\n",
        "2. **Decoder**: Generates output tokens one by one\n",
        "3. **First step**: Decoder starts with `<pad>` token (T5's special start token)\n",
        "4. **Output**: Model produces logits (probabilities) for each possible next token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare input\n",
        "input_sentence = \"translate english to german: hello, how are you?\"\n",
        "tokens = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
        "decoder_input_ids = torch.tensor([[tokenizer.tokenizer.pad_token_id]])\n",
        "\n",
        "print(f\"Input: '{input_sentence}'\")\n",
        "print(f\"Encoder input shape: {tokens['input_ids'].shape}\")\n",
        "print(f\"Decoder input (starting token): {decoder_input_ids.tolist()}\")\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    output = model(**tokens, decoder_input_ids=decoder_input_ids)\n",
        "\n",
        "print(f\"\\nâœ… Forward pass completed!\")\n",
        "print(f\"Output logits shape: {output.logits.shape}\")\n",
        "print(f\"   â†’ Shape means: [batch=1, sequence=1, vocab={model.vocab_size:,}]\")\n",
        "print(f\"   â†’ For each position, we have {model.vocab_size:,} logits (one per token)\")\n",
        "print(f\"\\nOutput contains:\")\n",
        "print(f\"  - logits: Probabilities for next token\")\n",
        "print(f\"  - past_key_values: Cached attention states\")\n",
        "print(f\"  - encoder_last_hidden_state: Final encoder representation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Text Generation\n",
        "\n",
        "**Autoregressive Generation:**\n",
        "Models generate text token by token. Each new token depends on all previous tokens.\n",
        "\n",
        "**Greedy Decoding:** Always picks the most likely next token (highest probability).\n",
        "\n",
        "The `model.generate()` method does this automatically with optimizations!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text using model.generate()\n",
        "# Note: We need to prepare tokens again for generation\n",
        "input_sentence = \"translate english to german: hello, how are you?\"\n",
        "tokens = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(**tokens, max_length=20)\n",
        "\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Input:  '{input_sentence}'\")\n",
        "print(f\"Output: '{generated_text}'\")\n",
        "print(f\"\\nðŸ’¡ The model translated English to German!\")\n",
        "print(f\"ðŸ’¡ model.generate() is optimized and faster than manual generation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Token Embeddings Visualization\n",
        "\n",
        "**What are embeddings?**\n",
        "- Each word/token is represented as a high-dimensional vector (512 dimensions for T5-small)\n",
        "- These vectors capture semantic meaning\n",
        "- Similar words have similar embeddings\n",
        "\n",
        "**Visualization techniques:**\n",
        "1. **PCA (Principal Component Analysis)**: Reduces 512D â†’ 2D for visualization\n",
        "2. **Cosine Similarity**: Measures how \"similar\" two word embeddings are (0-1 scale)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Words to visualize\n",
        "words = [\n",
        "    \"chair\",\n",
        "    \"table\",\n",
        "    \"plate\",\n",
        "    \"knife\",\n",
        "    \"spoon\",\n",
        "    \"horse\",\n",
        "    \"goat\",\n",
        "    \"sheep\",\n",
        "    \"cat\",\n",
        "    \"dog\",\n",
        "]\n",
        "\n",
        "print(f\"ðŸ“ Analyzing embeddings for {len(words)} words:\")\n",
        "for i, word in enumerate(words, 1):\n",
        "    print(f\"   {i}. {word}\")\n",
        "\n",
        "# Get token IDs for first token of each word\n",
        "word_tokens = tokenizer.encode(words, return_tensors=\"pt\", padding=True)[\"input_ids\"][:, 0]\n",
        "print(f\"\\nToken IDs: {word_tokens.tolist()}\")\n",
        "\n",
        "# Extract embeddings from the model's shared embedding layer\n",
        "with torch.no_grad():\n",
        "    token_embeddings = model.model.shared(word_tokens).cpu().detach().numpy()\n",
        "\n",
        "print(f\"âœ… Embeddings extracted!\")\n",
        "print(f\"   Shape: {token_embeddings.shape}\")\n",
        "print(f\"   â†’ Each word is a {token_embeddings.shape[1]}-dimensional vector\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PCA Visualization (2D Projection)\n",
        "\n",
        "PCA reduces high-dimensional embeddings to 2D so we can visualize them. Words that are semantically similar should appear close together!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply PCA to reduce dimensions from 512D to 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(token_embeddings)\n",
        "\n",
        "print(f\"Explained variance per component: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n",
        "print(\"â†’ This shows how much information is preserved in 2D\")\n",
        "\n",
        "# Create PCA plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], s=200, alpha=0.7, edgecolors='black', linewidth=1.5)\n",
        "\n",
        "# Add labels with better positioning\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, (X_pca[i, 0], X_pca[i, 1]), \n",
        "                xytext=(5, 5), textcoords='offset points', \n",
        "                fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.xlabel('First Principal Component', fontsize=13)\n",
        "plt.ylabel('Second Principal Component', fontsize=13)\n",
        "plt.title('Token Embeddings - PCA Visualization (2D Projection)\\nWords close together are semantically similar', \n",
        "          fontsize=15, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3, linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ’¡ Observations:\")\n",
        "print(\"   - Furniture words (chair, table, plate, knife, spoon) should cluster together\")\n",
        "print(\"   - Animal words (horse, goat, sheep, cat, dog) should cluster together\")\n",
        "print(\"   - This shows the model learned semantic relationships!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cosine Similarity Matrix\n",
        "\n",
        "Cosine similarity measures how similar two word embeddings are:\n",
        "- **1.0** = Identical (same word)\n",
        "- **0.8-1.0** = Very similar (e.g., \"cat\" and \"dog\")\n",
        "- **0.5-0.8** = Somewhat related\n",
        "- **0.0-0.5** = Unrelated\n",
        "- **Negative** = Opposites or very different\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute cosine similarity matrix\n",
        "similarity_matrix = cosine_similarity(token_embeddings)\n",
        "\n",
        "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
        "print(f\"â†’ Shows pairwise similarity between all {len(words)} words\\n\")\n",
        "\n",
        "# Create heatmap with better styling\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "cax = ax.imshow(similarity_matrix, cmap=\"RdYlBu_r\", vmin=-1, vmax=1, aspect='auto')\n",
        "cbar = fig.colorbar(cax, ax=ax, label='Cosine Similarity', shrink=0.8)\n",
        "cbar.ax.tick_params(labelsize=11)\n",
        "\n",
        "ax.set_xticks(range(len(words)))\n",
        "ax.set_yticks(range(len(words)))\n",
        "ax.set_xticklabels(words, rotation=45, ha='right', fontsize=11)\n",
        "ax.set_yticklabels(words, fontsize=11)\n",
        "ax.set_title('Cosine Similarity Matrix\\n(Red = similar, Blue = different)', \n",
        "             fontsize=15, fontweight='bold', pad=20)\n",
        "\n",
        "# Add similarity values to the plot (only show if similarity > 0.3 for readability)\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        sim_val = similarity_matrix[i, j]\n",
        "        # Use white text for dark backgrounds, black for light\n",
        "        text_color = 'white' if abs(sim_val) > 0.5 else 'black'\n",
        "        if abs(sim_val) > 0.3 or i == j:  # Show diagonal and significant similarities\n",
        "            ax.text(j, i, f'{sim_val:.2f}',\n",
        "                   ha=\"center\", va=\"center\", \n",
        "                   color=text_color, fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print most similar pairs\n",
        "print(\"\\nðŸ’¡ Most similar word pairs:\")\n",
        "similarities = []\n",
        "for i in range(len(words)):\n",
        "    for j in range(i + 1, len(words)):\n",
        "        similarities.append((words[i], words[j], similarity_matrix[i, j]))\n",
        "\n",
        "similarities.sort(key=lambda x: x[2], reverse=True)\n",
        "for i, (word1, word2, sim) in enumerate(similarities[:5], 1):\n",
        "    print(f\"   {i}. '{word1}' â†” '{word2}': {sim:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**What you learned:**\n",
        "1. âœ… **Tokenization**: Converting text to numbers (token IDs)\n",
        "2. âœ… **Model Architecture**: Understanding encoder-decoder structure\n",
        "3. âœ… **Forward Pass**: How models process input and generate output\n",
        "4. âœ… **Text Generation**: Autoregressive token-by-token generation\n",
        "5. âœ… **Embeddings**: How words are represented as vectors\n",
        "6. âœ… **Visualization**: PCA and cosine similarity for understanding embeddings\n",
        "\n",
        "**Next steps:**\n",
        "- Try different input prompts\n",
        "- Experiment with different T5 model sizes (t5-base, t5-large)\n",
        "- Explore attention mechanisms in `solution-02-attention.ipynb`\n",
        "- Modify the code to experiment with different models (BERT, GPT-2, etc.)\n",
        "\n",
        "**Resources:**\n",
        "- Full tutorial: `labs/solution-01-t5.ipynb`\n",
        "- Test suite: `test_t5_notebook.py`\n",
        "- More examples: `examples/` folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venvllm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
